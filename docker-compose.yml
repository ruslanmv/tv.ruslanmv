version: '3.8'

services:
  # ============================================
  # CORE INFRASTRUCTURE
  # ============================================
  
  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: tvruslanmv-db
    environment:
      POSTGRES_DB: tvruslanmv
      POSTGRES_USER: tvuser
      POSTGRES_PASSWORD: ${DB_PASSWORD:-changeme123}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql
      - ./database/seeds:/docker-entrypoint-initdb.d/seeds
    ports:
      - "5432:5432"
    networks:
      - tvruslanmv-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U tvuser -d tvruslanmv"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: tvruslanmv-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - tvruslanmv-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # OLLAMA - LOCAL LLM (DEFAULT)
  # ============================================
  
  ollama:
    image: ollama/ollama:latest
    container_name: tvruslanmv-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - tvruslanmv-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Ollama Model Loader (runs once to pull models)
  ollama-setup:
    image: ollama/ollama:latest
    container_name: tvruslanmv-ollama-setup
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - tvruslanmv-network
    environment:
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ./scripts/setup-ollama.sh:/setup.sh
    command: /bin/sh /setup.sh
    profiles:
      - setup

  # ============================================
  # BACKEND API
  # ============================================
  
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: tvruslanmv-backend
    environment:
      DATABASE_URL: postgresql://tvuser:${DB_PASSWORD:-changeme123}@postgres:5432/tvruslanmv
      REDIS_URL: redis://redis:6379/0
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma:2b}
      NEWS_LLM_MODEL: ${NEWS_LLM_MODEL:-ollama/gemma:2b}
      # Optional: watsonx.ai for better quality
      WATSONX_APIKEY: ${WATSONX_APIKEY}
      WATSONX_URL: ${WATSONX_URL}
      WATSONX_PROJECT_ID: ${WATSONX_PROJECT_ID}
      # Optional: YouTube
      YOUTUBE_API_KEY: ${YOUTUBE_API_KEY}
      YOUTUBE_CLIENT_ID: ${YOUTUBE_CLIENT_ID}
      YOUTUBE_CLIENT_SECRET: ${YOUTUBE_CLIENT_SECRET}
      YOUTUBE_REFRESH_TOKEN: ${YOUTUBE_REFRESH_TOKEN}
      ENVIRONMENT: ${ENVIRONMENT:-development}
    volumes:
      - ./backend:/app
      - backend_uploads:/app/uploads
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - tvruslanmv-network
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # ============================================
  # MCP SERVER FOR AI AGENTS
  # ============================================
  
  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: tvruslanmv-mcp
    environment:
      API_BASE_URL: http://backend:8000
      MCP_SERVER_PORT: 3000
      NODE_ENV: ${NODE_ENV:-development}
    volumes:
      - ./mcp-server:/app
      - /app/node_modules
    ports:
      - "3000:3000"
    depends_on:
      - backend
    networks:
      - tvruslanmv-network
    command: npm run dev

  # ============================================
  # FRONTEND
  # ============================================
  
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: development
    container_name: tvruslanmv-frontend
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      NEXT_PUBLIC_MCP_WS_URL: ${NEXT_PUBLIC_MCP_WS_URL:-ws://localhost:3000}
      NEXT_PUBLIC_YOUTUBE_API_KEY: ${YOUTUBE_API_KEY}
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    ports:
      - "3001:3000"
    depends_on:
      - backend
      - mcp-server
    networks:
      - tvruslanmv-network
    command: npm run dev

  # ============================================
  # CONTENT GENERATOR (Manual/Scheduled)
  # ============================================
  
  content-generator:
    build:
      context: ./content-generator
      dockerfile: Dockerfile
    container_name: tvruslanmv-generator
    environment:
      DATABASE_URL: postgresql://tvuser:${DB_PASSWORD:-changeme123}@postgres:5432/tvruslanmv
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma:2b}
      NEWS_LLM_MODEL: ${NEWS_LLM_MODEL:-ollama/gemma:2b}
      NEWS_LLM_TEMPERATURE: ${NEWS_LLM_TEMPERATURE:-0.7}
      # Optional: watsonx.ai
      WATSONX_APIKEY: ${WATSONX_APIKEY}
      WATSONX_URL: ${WATSONX_URL}
      WATSONX_PROJECT_ID: ${WATSONX_PROJECT_ID}
      # Optional: Other providers
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
    volumes:
      - ./content-generator:/app
      - ./scripts:/app/scripts
      - generator_output:/app/output
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - tvruslanmv-network
    profiles:
      - tools
    command: python src/main.py --mode daemon

  # ============================================
  # VIDEO PROCESSOR
  # ============================================
  
  video-processor:
    build:
      context: ./video-processor
      dockerfile: Dockerfile
    container_name: tvruslanmv-video
    environment:
      YOUTUBE_CLIENT_ID: ${YOUTUBE_CLIENT_ID}
      YOUTUBE_CLIENT_SECRET: ${YOUTUBE_CLIENT_SECRET}
      YOUTUBE_REFRESH_TOKEN: ${YOUTUBE_REFRESH_TOKEN}
      ELEVENLABS_API_KEY: ${ELEVENLABS_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OUTPUT_DIR: /app/output
    volumes:
      - ./video-processor:/app
      - video_output:/app/output
      - ./video-processor/assets:/app/assets
    networks:
      - tvruslanmv-network
    profiles:
      - tools
    command: python src/main.py --mode daemon

  # ============================================
  # NGINX REVERSE PROXY (Optional)
  # ============================================
  
  nginx:
    image: nginx:alpine
    container_name: tvruslanmv-nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - frontend
      - backend
      - mcp-server
    networks:
      - tvruslanmv-network
    profiles:
      - production

# ============================================
# VOLUMES
# ============================================

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local
  backend_uploads:
    driver: local
  generator_output:
    driver: local
  video_output:
    driver: local

# ============================================
# NETWORKS
# ============================================

networks:
  tvruslanmv-network:
    driver: bridge

# ============================================
# USAGE EXAMPLES
# ============================================
#
# Start all services:
#   docker-compose up -d
#
# Setup Ollama models:
#   docker-compose --profile setup up ollama-setup
#
# Generate episode manually:
#   docker-compose run --rm content-generator python scripts/generate_script.py
#
# Run content generator daemon:
#   docker-compose --profile tools up content-generator
#
# View logs:
#   docker-compose logs -f ollama
#   docker-compose logs -f backend
#
# Stop all services:
#   docker-compose down
#
# Clean everything:
#   docker-compose down -v
# ============================================
